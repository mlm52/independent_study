{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import prettyNEAT as pn\n",
    "import domain\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import PIL\n",
    "import math\n",
    "from collections import namedtuple\n",
    "from torch import nn\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "import pandas as pd\n",
    "from ribs.visualize import grid_archive_heatmap\n",
    "import sys\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=np.linspace(-3.9375,3.9375,32)\n",
    "x= x.repeat(32)\n",
    "x=x.reshape((32,32))\n",
    "x=np.stack((x,x.transpose()),axis=2)\n",
    "r=np.sqrt(np.square(x).sum(axis=2))\n",
    "x=np.append(x,np.expand_dims(r,2),axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1c399d42eb0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "na=[[1,0,0,1,0,0,0,0],\n",
    "   [1,1,0,1,0,0,0,0],\n",
    "   [1,0,1,1,0,0,0,0],\n",
    "   [1,0,0,1,0,0,0,0],\n",
    "   [0,0,0,0,0,1,0,0],\n",
    "   [0,0,0,0,1,0,1,0],\n",
    "   [0,0,0,1,1,1,1,1],\n",
    "   [0,0,0,1,0,0,0,1]]\n",
    "im = PIL.Image.open(\"simple.png\")\n",
    "im1=im.convert(\"L\").resize((32,32))\n",
    "goal=np.array(im1)/255\n",
    "goal=np.expand_dims(goal,2)\n",
    "im.close()\n",
    "plt.imshow(goal, cmap='Greys_r',vmin=0, vmax=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getMyFitness(game, wVec, aVec, nRep=False,):\n",
    "    if nRep is False:\n",
    "        nRep = game.nReps\n",
    "    wVec[np.isnan(wVec)] = 0\n",
    "    reward = np.empty(nRep)\n",
    "    for iRep in range(nRep):\n",
    "        reward[iRep] = myTestInd(game, wVec, aVec)\n",
    "    fitness = np.mean(reward)\n",
    "    return fitness\n",
    "\n",
    "def myTestInd(game, wVec, aVec):\n",
    "    myPic = np.empty((32,32,1))      \n",
    "    for i in range(game.maxEpisodeLength): \n",
    "        myPic[i%32][int(i/32)] = pn.act(wVec, aVec, game.nInput, game.nOutput, x[i%32][int(i/32)])\n",
    "    return (np.square(goal-myPic).sum()-(32*32))/((-32*32))*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "Game = namedtuple('Game', ['env_name', 'time_factor', 'actionSelect',\n",
    "  'input_size', 'output_size', 'layers', 'i_act', 'h_act',\n",
    "  'o_act', 'weightCap','noise_bias','output_noise','max_episode_length','in_out_labels'])\n",
    "\n",
    "game= Game(env_name='cppn',\n",
    "  actionSelect='all', # all, soft, hard\n",
    "  input_size=3,\n",
    "  output_size=1,\n",
    "  time_factor=0,\n",
    "  layers=[64, 64],\n",
    "  i_act=np.full(3,1),\n",
    "  h_act=[1,3,4,10],\n",
    "  o_act=np.full(1,6),\n",
    "  weightCap = 25,\n",
    "  noise_bias=0.0,\n",
    "  max_episode_length = 32*32,\n",
    "  output_noise=[False],\n",
    "  in_out_labels = ['Pixel Intensity'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyp = domain.loadHyp(pFileName='C:\\\\Users\\\\mmkir\\\\Documents\\\\485\\\\indeptant study\\\\prettyNEAT\\\\config\\\\default_neat.json')\n",
    "hyp['alg_nReps']=1\n",
    "task = domain.GymTask(game, paramOnly=True, nReps=hyp['alg_nReps'])\n",
    "hyp['ann_nInput']   = task.nInput\n",
    "hyp['ann_nOutput']  = task.nOutput\n",
    "hyp['ann_initAct']  = task.activations\n",
    "hyp['ann_absWCap']  = task.absWCap\n",
    "hyp['ann_mutSigma'] = task.absWCap * 0.2\n",
    "hyp['ann_layers']   = task.layers\n",
    "hyp['ann_actRange'] = task.actRange\n",
    "hyp['bc_range']=[[0,64],[0,31]]\n",
    "hyp['map_size']=[16,16]\n",
    "hyp['maxGen']=10000\n",
    "hyp['pop_size']=512\n",
    "hyp['prob_mutAct']= 0.05\n",
    "hyp['prob_addNode']= 0.06\n",
    "hyp['prob_addConn']= 0.08\n",
    "hyp[\"select_tournSize\"]= 128\n",
    "neat = pn.Neat(hyp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gen:0, avg fitness:69.155\n",
      "gen:25, avg fitness:73.596\n",
      "gen:50, avg fitness:72.315\n",
      "gen:75, avg fitness:74.312\n",
      "gen:100, avg fitness:74.038\n",
      "gen:125, avg fitness:72.751\n",
      "gen:150, avg fitness:73.437\n",
      "gen:175, avg fitness:73.791\n",
      "gen:200, avg fitness:71.809\n",
      "gen:225, avg fitness:72.776\n",
      "gen:250, avg fitness:71.907\n",
      "gen:275, avg fitness:72.513\n",
      "gen:300, avg fitness:72.755\n",
      "gen:325, avg fitness:72.521\n",
      "gen:350, avg fitness:73.443\n",
      "gen:375, avg fitness:72.908\n",
      "gen:400, avg fitness:71.441\n",
      "gen:425, avg fitness:73.492\n",
      "gen:450, avg fitness:72.805\n",
      "gen:475, avg fitness:74.558\n",
      "gen:500, avg fitness:73.306\n",
      "gen:525, avg fitness:72.844\n",
      "gen:550, avg fitness:72.393\n",
      "gen:575, avg fitness:72.246\n",
      "gen:600, avg fitness:73.989\n",
      "gen:625, avg fitness:73.840\n",
      "gen:650, avg fitness:73.604\n",
      "gen:675, avg fitness:73.542\n",
      "gen:700, avg fitness:73.779\n",
      "gen:725, avg fitness:72.178\n",
      "gen:750, avg fitness:73.985\n",
      "gen:775, avg fitness:73.904\n",
      "gen:800, avg fitness:74.410\n",
      "gen:825, avg fitness:73.967\n",
      "gen:850, avg fitness:72.811\n",
      "gen:875, avg fitness:72.735\n",
      "gen:900, avg fitness:73.500\n",
      "gen:925, avg fitness:72.940\n",
      "gen:950, avg fitness:72.442\n",
      "gen:975, avg fitness:73.550\n",
      "gen:1000, avg fitness:71.888\n",
      "gen:1025, avg fitness:73.503\n",
      "gen:1050, avg fitness:71.243\n",
      "gen:1075, avg fitness:71.983\n",
      "gen:1100, avg fitness:72.503\n",
      "gen:1125, avg fitness:72.718\n",
      "gen:1150, avg fitness:72.111\n",
      "gen:1175, avg fitness:72.305\n",
      "gen:1200, avg fitness:73.545\n",
      "gen:1225, avg fitness:73.387\n",
      "gen:1250, avg fitness:72.710\n",
      "gen:1275, avg fitness:72.894\n",
      "gen:1300, avg fitness:73.402\n",
      "gen:1325, avg fitness:73.903\n",
      "gen:1350, avg fitness:74.004\n",
      "gen:1375, avg fitness:74.211\n",
      "gen:1400, avg fitness:72.325\n",
      "gen:1425, avg fitness:72.648\n",
      "gen:1450, avg fitness:74.280\n",
      "gen:1475, avg fitness:74.264\n",
      "gen:1500, avg fitness:72.917\n",
      "gen:1525, avg fitness:71.622\n",
      "gen:1550, avg fitness:72.436\n",
      "gen:1575, avg fitness:72.043\n",
      "gen:1600, avg fitness:72.031\n",
      "gen:1625, avg fitness:71.834\n",
      "gen:1650, avg fitness:70.387\n",
      "gen:1675, avg fitness:71.718\n",
      "gen:1700, avg fitness:70.978\n",
      "gen:1725, avg fitness:70.794\n",
      "gen:1750, avg fitness:70.448\n",
      "gen:1775, avg fitness:71.657\n",
      "gen:1800, avg fitness:71.861\n",
      "gen:1825, avg fitness:71.354\n",
      "gen:1850, avg fitness:70.504\n",
      "gen:1875, avg fitness:70.448\n",
      "gen:1900, avg fitness:71.159\n",
      "gen:1925, avg fitness:71.043\n",
      "gen:1950, avg fitness:71.584\n",
      "gen:1975, avg fitness:71.469\n",
      "gen:2000, avg fitness:71.407\n",
      "gen:2025, avg fitness:71.264\n",
      "gen:2050, avg fitness:72.043\n",
      "gen:2075, avg fitness:71.812\n",
      "gen:2100, avg fitness:70.899\n",
      "gen:2125, avg fitness:70.809\n",
      "gen:2150, avg fitness:71.993\n",
      "gen:2175, avg fitness:71.160\n",
      "gen:2200, avg fitness:71.552\n",
      "gen:2225, avg fitness:72.328\n",
      "gen:2250, avg fitness:72.935\n",
      "gen:2275, avg fitness:71.873\n",
      "gen:2300, avg fitness:71.834\n",
      "gen:2325, avg fitness:72.849\n",
      "gen:2350, avg fitness:72.250\n",
      "gen:2375, avg fitness:71.722\n",
      "gen:2400, avg fitness:72.963\n",
      "gen:2425, avg fitness:72.870\n",
      "gen:2450, avg fitness:71.961\n",
      "gen:2475, avg fitness:72.160\n",
      "gen:2500, avg fitness:72.661\n",
      "gen:2525, avg fitness:71.244\n",
      "gen:2550, avg fitness:71.938\n",
      "gen:2575, avg fitness:72.658\n",
      "gen:2600, avg fitness:72.595\n",
      "gen:2625, avg fitness:71.157\n",
      "gen:2650, avg fitness:73.276\n",
      "gen:2675, avg fitness:73.139\n",
      "gen:2700, avg fitness:72.991\n",
      "gen:2725, avg fitness:72.693\n",
      "gen:2750, avg fitness:73.427\n",
      "gen:2775, avg fitness:74.038\n",
      "gen:2800, avg fitness:73.928\n",
      "gen:2825, avg fitness:73.859\n",
      "gen:2850, avg fitness:73.120\n",
      "gen:2875, avg fitness:73.883\n",
      "gen:2900, avg fitness:72.440\n",
      "gen:2925, avg fitness:73.895\n",
      "gen:2950, avg fitness:73.713\n",
      "gen:2975, avg fitness:73.826\n",
      "gen:3000, avg fitness:73.152\n",
      "gen:3025, avg fitness:73.460\n",
      "gen:3050, avg fitness:73.821\n",
      "gen:3075, avg fitness:74.109\n",
      "gen:3100, avg fitness:72.602\n",
      "gen:3125, avg fitness:73.964\n",
      "gen:3150, avg fitness:73.612\n",
      "gen:3175, avg fitness:73.952\n",
      "gen:3200, avg fitness:73.902\n",
      "gen:3225, avg fitness:73.859\n",
      "gen:3250, avg fitness:73.549\n",
      "gen:3275, avg fitness:73.097\n",
      "gen:3300, avg fitness:71.997\n",
      "gen:3325, avg fitness:72.036\n",
      "gen:3350, avg fitness:73.082\n",
      "gen:3375, avg fitness:73.362\n",
      "gen:3400, avg fitness:73.969\n",
      "gen:3425, avg fitness:73.606\n",
      "gen:3450, avg fitness:73.820\n",
      "gen:3475, avg fitness:73.619\n",
      "gen:3500, avg fitness:73.696\n",
      "gen:3525, avg fitness:72.186\n",
      "gen:3550, avg fitness:73.569\n",
      "gen:3575, avg fitness:73.569\n",
      "gen:3600, avg fitness:73.778\n",
      "gen:3625, avg fitness:73.960\n",
      "gen:3650, avg fitness:73.314\n",
      "gen:3675, avg fitness:74.333\n",
      "gen:3700, avg fitness:73.918\n",
      "gen:3725, avg fitness:73.185\n",
      "gen:3750, avg fitness:73.673\n",
      "gen:3775, avg fitness:73.924\n",
      "gen:3800, avg fitness:73.768\n",
      "gen:3825, avg fitness:73.104\n",
      "gen:3850, avg fitness:74.203\n",
      "gen:3875, avg fitness:72.712\n",
      "gen:3900, avg fitness:72.412\n",
      "gen:3925, avg fitness:71.773\n",
      "gen:3950, avg fitness:73.952\n",
      "gen:3975, avg fitness:73.194\n",
      "gen:4000, avg fitness:73.647\n",
      "gen:4025, avg fitness:74.058\n",
      "gen:4050, avg fitness:73.924\n",
      "gen:4075, avg fitness:73.300\n",
      "gen:4100, avg fitness:74.029\n",
      "gen:4125, avg fitness:73.032\n",
      "gen:4150, avg fitness:74.008\n",
      "gen:4175, avg fitness:72.640\n",
      "gen:4200, avg fitness:72.675\n",
      "gen:4225, avg fitness:73.776\n",
      "gen:4250, avg fitness:73.091\n",
      "gen:4275, avg fitness:74.084\n",
      "gen:4300, avg fitness:72.227\n",
      "gen:4325, avg fitness:72.931\n",
      "gen:4350, avg fitness:73.395\n",
      "gen:4375, avg fitness:73.430\n",
      "gen:4400, avg fitness:74.031\n",
      "gen:4425, avg fitness:74.156\n",
      "gen:4450, avg fitness:73.658\n",
      "gen:4475, avg fitness:71.851\n",
      "gen:4500, avg fitness:73.899\n",
      "gen:4525, avg fitness:72.931\n",
      "gen:4550, avg fitness:73.277\n",
      "gen:4575, avg fitness:74.153\n",
      "gen:4600, avg fitness:72.862\n",
      "gen:4625, avg fitness:71.677\n",
      "gen:4650, avg fitness:72.829\n",
      "gen:4675, avg fitness:74.111\n",
      "gen:4700, avg fitness:72.794\n",
      "gen:4725, avg fitness:72.854\n",
      "gen:4750, avg fitness:72.577\n",
      "gen:4775, avg fitness:73.915\n",
      "gen:4800, avg fitness:73.369\n",
      "gen:4825, avg fitness:73.969\n",
      "gen:4850, avg fitness:73.280\n",
      "gen:4875, avg fitness:73.034\n",
      "gen:4900, avg fitness:74.179\n",
      "gen:4925, avg fitness:73.140\n",
      "gen:4950, avg fitness:73.665\n",
      "gen:4975, avg fitness:71.947\n",
      "gen:5000, avg fitness:72.253\n",
      "gen:5025, avg fitness:72.397\n",
      "gen:5050, avg fitness:73.179\n",
      "gen:5075, avg fitness:71.393\n",
      "gen:5100, avg fitness:73.048\n",
      "gen:5125, avg fitness:72.264\n",
      "gen:5150, avg fitness:72.339\n",
      "gen:5175, avg fitness:72.764\n",
      "gen:5200, avg fitness:72.463\n",
      "gen:5225, avg fitness:72.899\n",
      "gen:5250, avg fitness:72.143\n",
      "gen:5275, avg fitness:72.547\n",
      "gen:5300, avg fitness:72.332\n",
      "gen:5325, avg fitness:73.511\n",
      "gen:5350, avg fitness:72.892\n",
      "gen:5375, avg fitness:73.284\n",
      "gen:5400, avg fitness:73.928\n",
      "gen:5425, avg fitness:72.776\n",
      "gen:5450, avg fitness:72.614\n",
      "gen:5475, avg fitness:71.560\n",
      "gen:5500, avg fitness:73.286\n",
      "gen:5525, avg fitness:73.189\n",
      "gen:5550, avg fitness:73.860\n",
      "gen:5575, avg fitness:73.544\n",
      "gen:5600, avg fitness:73.706\n",
      "gen:5625, avg fitness:73.520\n",
      "gen:5650, avg fitness:74.442\n",
      "gen:5675, avg fitness:73.554\n",
      "gen:5700, avg fitness:73.139\n",
      "gen:5725, avg fitness:72.944\n",
      "gen:5750, avg fitness:73.447\n",
      "gen:5775, avg fitness:73.040\n",
      "gen:5800, avg fitness:72.280\n",
      "gen:5825, avg fitness:72.922\n",
      "gen:5850, avg fitness:72.747\n",
      "gen:5875, avg fitness:73.162\n",
      "gen:5900, avg fitness:73.233\n",
      "gen:5925, avg fitness:73.090\n",
      "gen:5950, avg fitness:73.511\n",
      "gen:5975, avg fitness:73.381\n",
      "gen:6000, avg fitness:73.585\n",
      "gen:6025, avg fitness:72.854\n",
      "gen:6050, avg fitness:73.106\n",
      "gen:6075, avg fitness:73.595\n",
      "gen:6100, avg fitness:73.350\n",
      "gen:6125, avg fitness:72.427\n",
      "gen:6150, avg fitness:73.079\n",
      "gen:6175, avg fitness:72.592\n",
      "gen:6200, avg fitness:73.985\n",
      "gen:6225, avg fitness:73.215\n",
      "gen:6250, avg fitness:72.925\n",
      "gen:6275, avg fitness:73.117\n",
      "gen:6300, avg fitness:73.350\n",
      "gen:6325, avg fitness:73.250\n",
      "gen:6350, avg fitness:73.265\n",
      "gen:6375, avg fitness:73.064\n",
      "gen:6400, avg fitness:73.663\n",
      "gen:6425, avg fitness:72.885\n",
      "gen:6450, avg fitness:72.732\n",
      "gen:6475, avg fitness:73.443\n",
      "gen:6500, avg fitness:73.500\n",
      "gen:6525, avg fitness:73.584\n",
      "gen:6550, avg fitness:73.037\n",
      "gen:6575, avg fitness:73.327\n",
      "gen:6600, avg fitness:72.667\n",
      "gen:6625, avg fitness:73.338\n",
      "gen:6650, avg fitness:73.170\n",
      "gen:6675, avg fitness:72.656\n",
      "gen:6700, avg fitness:72.282\n",
      "gen:6725, avg fitness:73.947\n",
      "gen:6750, avg fitness:73.582\n",
      "gen:6775, avg fitness:73.518\n",
      "gen:6800, avg fitness:74.000\n",
      "gen:6825, avg fitness:73.937\n",
      "gen:6850, avg fitness:74.024\n",
      "gen:6875, avg fitness:73.179\n",
      "gen:6900, avg fitness:73.617\n",
      "gen:6925, avg fitness:73.668\n",
      "gen:6950, avg fitness:72.027\n",
      "gen:6975, avg fitness:73.451\n",
      "gen:7000, avg fitness:73.302\n",
      "gen:7025, avg fitness:72.941\n",
      "gen:7050, avg fitness:73.270\n",
      "gen:7075, avg fitness:73.022\n",
      "gen:7100, avg fitness:73.659\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gen:7125, avg fitness:73.736\n",
      "gen:7150, avg fitness:72.643\n",
      "gen:7175, avg fitness:72.768\n",
      "gen:7200, avg fitness:73.375\n",
      "gen:7225, avg fitness:72.861\n",
      "gen:7250, avg fitness:72.464\n",
      "gen:7275, avg fitness:72.789\n",
      "gen:7300, avg fitness:73.519\n",
      "gen:7325, avg fitness:73.236\n",
      "gen:7350, avg fitness:73.769\n",
      "gen:7375, avg fitness:73.580\n",
      "gen:7400, avg fitness:73.441\n",
      "gen:7425, avg fitness:73.594\n",
      "gen:7450, avg fitness:72.537\n",
      "gen:7475, avg fitness:71.554\n",
      "gen:7500, avg fitness:73.428\n",
      "gen:7525, avg fitness:73.679\n",
      "gen:7550, avg fitness:73.478\n",
      "gen:7575, avg fitness:73.566\n",
      "gen:7600, avg fitness:74.283\n",
      "gen:7625, avg fitness:73.331\n",
      "gen:7650, avg fitness:73.600\n",
      "gen:7675, avg fitness:73.322\n",
      "gen:7700, avg fitness:73.262\n",
      "gen:7725, avg fitness:73.171\n",
      "gen:7750, avg fitness:74.276\n",
      "gen:7775, avg fitness:72.929\n",
      "gen:7800, avg fitness:73.804\n",
      "gen:7825, avg fitness:72.570\n",
      "gen:7850, avg fitness:73.630\n",
      "gen:7875, avg fitness:73.072\n",
      "gen:7900, avg fitness:74.095\n",
      "gen:7925, avg fitness:73.184\n",
      "gen:7950, avg fitness:73.280\n",
      "gen:7975, avg fitness:72.377\n",
      "gen:8000, avg fitness:73.083\n",
      "gen:8025, avg fitness:73.311\n",
      "gen:8050, avg fitness:73.541\n",
      "gen:8075, avg fitness:72.619\n",
      "gen:8100, avg fitness:72.577\n",
      "gen:8125, avg fitness:72.998\n",
      "gen:8150, avg fitness:73.397\n",
      "gen:8175, avg fitness:73.557\n",
      "gen:8200, avg fitness:73.242\n",
      "gen:8225, avg fitness:73.360\n",
      "gen:8250, avg fitness:73.462\n",
      "gen:8275, avg fitness:73.372\n",
      "gen:8300, avg fitness:73.212\n",
      "gen:8325, avg fitness:72.667\n",
      "gen:8350, avg fitness:72.502\n",
      "gen:8375, avg fitness:73.710\n",
      "gen:8400, avg fitness:72.176\n",
      "gen:8425, avg fitness:73.647\n",
      "gen:8450, avg fitness:73.260\n",
      "gen:8475, avg fitness:74.207\n",
      "gen:8500, avg fitness:72.656\n",
      "gen:8525, avg fitness:72.738\n",
      "gen:8550, avg fitness:73.765\n",
      "gen:8575, avg fitness:74.136\n",
      "gen:8600, avg fitness:73.171\n",
      "gen:8625, avg fitness:74.149\n",
      "gen:8650, avg fitness:72.477\n",
      "gen:8675, avg fitness:73.082\n",
      "gen:8700, avg fitness:72.537\n",
      "gen:8725, avg fitness:73.919\n",
      "gen:8750, avg fitness:72.417\n",
      "gen:8775, avg fitness:73.643\n",
      "gen:8800, avg fitness:73.716\n",
      "gen:8825, avg fitness:73.756\n",
      "gen:8850, avg fitness:73.233\n",
      "gen:8875, avg fitness:72.074\n",
      "gen:8900, avg fitness:73.261\n",
      "gen:8925, avg fitness:73.468\n",
      "gen:8950, avg fitness:73.678\n",
      "gen:8975, avg fitness:73.716\n",
      "gen:9000, avg fitness:73.721\n",
      "gen:9025, avg fitness:73.825\n",
      "gen:9050, avg fitness:74.031\n",
      "gen:9075, avg fitness:72.650\n",
      "gen:9100, avg fitness:73.312\n",
      "gen:9125, avg fitness:73.706\n",
      "gen:9150, avg fitness:73.662\n",
      "gen:9175, avg fitness:73.964\n",
      "gen:9200, avg fitness:73.277\n",
      "gen:9225, avg fitness:73.429\n",
      "gen:9250, avg fitness:73.858\n",
      "gen:9275, avg fitness:72.802\n",
      "gen:9300, avg fitness:72.674\n",
      "gen:9325, avg fitness:73.527\n",
      "gen:9350, avg fitness:73.296\n",
      "gen:9375, avg fitness:73.019\n",
      "gen:9400, avg fitness:73.279\n",
      "gen:9425, avg fitness:72.721\n",
      "gen:9450, avg fitness:72.863\n",
      "gen:9475, avg fitness:73.711\n",
      "gen:9500, avg fitness:74.117\n",
      "gen:9525, avg fitness:73.821\n",
      "gen:9550, avg fitness:72.978\n",
      "gen:9575, avg fitness:72.544\n",
      "gen:9600, avg fitness:73.390\n",
      "gen:9625, avg fitness:73.905\n",
      "gen:9650, avg fitness:73.444\n",
      "gen:9675, avg fitness:73.936\n",
      "gen:9700, avg fitness:72.024\n",
      "gen:9725, avg fitness:72.887\n",
      "gen:9750, avg fitness:73.191\n",
      "gen:9775, avg fitness:73.771\n",
      "gen:9800, avg fitness:72.849\n",
      "gen:9825, avg fitness:73.534\n",
      "gen:9850, avg fitness:73.487\n",
      "gen:9875, avg fitness:74.028\n",
      "gen:9900, avg fitness:73.087\n",
      "gen:9925, avg fitness:73.435\n",
      "gen:9950, avg fitness:73.059\n",
      "gen:9975, avg fitness:72.572\n"
     ]
    }
   ],
   "source": [
    "neat = pn.Neat(hyp)\n",
    "for gen in range(hyp['maxGen']):\n",
    "    pop = neat.ask()\n",
    "# Get newly evolved individuals from NEAT\n",
    "    reward = np.empty(len(pop), dtype=np.float64)\n",
    "    for i in range(len(pop)):\n",
    "        task = domain.GymTask(game, paramOnly=True, nReps=hyp['alg_nReps'])\n",
    "        wVec   = pop[i].wMat.flatten()\n",
    "        aVec   = pop[i].aVec.flatten()\n",
    "        reward[i] = getMyFitness(task, wVec, aVec) # process it\n",
    "    neat.tell(reward)\n",
    "    neat.gen += 1\n",
    "    if gen % 25 == 0:\n",
    "        my_max=(np.mean(reward))\n",
    "        print(f'gen:{gen}, avg fitness:{my_max:.3f}')\n",
    "        if my_max > 99.5:\n",
    "            print(\"sucess\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeatME(pn.Neat):\n",
    "    def __init__(self,hyp):\n",
    "        super().__init__(hyp)\n",
    "        self.map_elites={}\n",
    "    def ask(self):\n",
    "    \n",
    "        if len(self.pop) == 0:\n",
    "            self.initPop()      # Initialize population\n",
    "        else:\n",
    "            self.probMoo() # Rank population according to objectivess\n",
    "            self.fill_elites()\n",
    "            self.gen_with_elites()# Create child population\n",
    "        return self.pop\n",
    "    \n",
    "    def tell(self,reward,bc0,bc1):\n",
    "        super().tell(reward)\n",
    "        for i in range(np.shape(reward)[0]):\n",
    "            #self.pop[i].bc0=bc0[i]\n",
    "            self.pop[i].bc1=bc1[i]\n",
    "\n",
    "   \n",
    "    def fill_elites(self, to_fill=None):\n",
    "        if to_fill is None:\n",
    "            to_fill = self.pop\n",
    "        map_size = self.p[\"map_size\"]\n",
    "        bc_range = self.p['bc_range']\n",
    "        for my_ind in to_fill:\n",
    "            adjusted_bc0=math.ceil((my_ind.bc0-bc_range[0][0])/(bc_range[0][1]-bc_range[0][0])*map_size[0])-1\n",
    "            adjusted_bc1=math.ceil((my_ind.bc1-bc_range[1][0])/(bc_range[1][1]-bc_range[1][0])*map_size[1])-1\n",
    "            \n",
    "            if adjusted_bc0>map_size[0]-1:\n",
    "                adjusted_bc0 = map_size[0]-1\n",
    "            elif adjusted_bc0<0:\n",
    "                adjusted_bc0 =0\n",
    "                \n",
    "            if adjusted_bc1>map_size[1]-1:\n",
    "                adjusted_bc1 = map_size[1]-1\n",
    "            elif adjusted_bc1<0:\n",
    "                adjusted_bc1 =0 \n",
    "                \n",
    "            key =adjusted_bc0*map_size[1] + adjusted_bc1\n",
    "            if key in self.map_elites:\n",
    "                if self.map_elites[key].fitness<my_ind.fitness:\n",
    "                    self.map_elites[key]=my_ind\n",
    "            else:\n",
    "                self.map_elites[key]=my_ind\n",
    "    \n",
    "    def gen_with_elites(self):\n",
    "        new_pop = []\n",
    "        pop_size = neat.p['popSize']\n",
    "        numberToCull = int(np.floor(self.p['select_cullRatio'] * len(neat.pop)))\n",
    "        if numberToCull > 0:\n",
    "            neat.pop[-numberToCull:] = []\n",
    "        parentB=np.random.randint(len(neat.pop),size=(pop_size,neat.p['select_tournSize']))\n",
    "        parentB = np.min(parentB,1)\n",
    "        a_list=[*self.map_elites.values()]\n",
    "        for i in range(pop_size):\n",
    "            parentA=random.choice(a_list)\n",
    "            if np.random.rand() > self.p['prob_crossover']:\n",
    "                child, self.innov = parentA.createChild(neat.p,neat.innov,neat.gen)\n",
    "            else:\n",
    "                child, self.innov = parentA.createChild(neat.p, neat.innov, neat.gen, mate=neat.pop[parentB[i]])\n",
    "            child.express()\n",
    "            new_pop.append(child)\n",
    "        self.pop=new_pop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, encoded_space_dim):\n",
    "        super().__init__()\n",
    "        \n",
    "        ### Convolutional section\n",
    "        self.encoder_cnn = nn.Sequential(\n",
    "            nn.Conv2d(1, 8, 3, stride=2, padding=1),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(8, 16, 3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(16, 32, 3, stride=2, padding=0),\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "        \n",
    "        ### Flatten layer\n",
    "        self.flatten = nn.Flatten(start_dim=1)\n",
    "### Linear section\n",
    "        self.encoder_lin = nn.Sequential(\n",
    "            nn.Linear(3 * 3 * 32, 128),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(128, encoded_space_dim)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.encoder_cnn(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.encoder_lin(x)\n",
    "        return x\n",
    "class Decoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, encoded_space_dim):\n",
    "        super().__init__()\n",
    "        self.decoder_lin = nn.Sequential(\n",
    "            nn.Linear(encoded_space_dim, 128),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(128, 3 * 3 * 32),\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "\n",
    "        self.unflatten = nn.Unflatten(dim=1, \n",
    "        unflattened_size=(32, 3, 3))\n",
    "\n",
    "        self.decoder_conv = nn.Sequential(\n",
    "            nn.ConvTranspose2d(32, 16, 4, \n",
    "            stride=2, output_padding=0),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(16, 8, 4, stride=2, \n",
    "            padding=1, output_padding=0),\n",
    "            nn.BatchNorm2d(8),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(8, 1, 4, stride=2, \n",
    "            padding=1, output_padding=0)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.decoder_lin(x)\n",
    "        x = self.unflatten(x)\n",
    "        x = self.decoder_conv(x)\n",
    "        x = torch.sigmoid(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected device: cpu\n"
     ]
    }
   ],
   "source": [
    "### Define the loss function\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "\n",
    "### Define an optimizer (both for the encoder and the decoder!)\n",
    "lr= 0.001\n",
    "\n",
    "### Set the random seed for reproducible results\n",
    "torch.manual_seed(0)\n",
    "\n",
    "### Initialize the two networks\n",
    "d = 4\n",
    "\n",
    "#model = Autoencoder(encoded_space_dim=encoded_space_dim)\n",
    "encoder = Encoder(encoded_space_dim=d)\n",
    "decoder = Decoder(encoded_space_dim=d)\n",
    "params_to_optimize = [\n",
    "    {'params': encoder.parameters()},\n",
    "    {'params': decoder.parameters()}\n",
    "]\n",
    "\n",
    "optim = torch.optim.Adam(params_to_optimize, lr=lr, weight_decay=1e-05)\n",
    "\n",
    "# Check if the GPU is available\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(f'Selected device: {device}')\n",
    "\n",
    "# Move both the encoder and the decoder to the selected device\n",
    "encoder.to(device)\n",
    "decoder.to(device)\n",
    "encoder.float()\n",
    "decoder.float()\n",
    "def train_epoch_den(encoder, decoder, device, archive, loss_fn, optimizer,noise_factor=0.3):\n",
    "    # Set train mode for both the encoder and the decoder\n",
    "    encoder.train()\n",
    "    decoder.train()\n",
    "    train_loss = []\n",
    "    # Iterate the dataloader (we do not need the label values, this is unsupervised learning)\n",
    "    for image_batch in archive: # with \"_\" we just ignore the labels (the second element of the dataloader tuple)\n",
    "        # Move tensor to the proper device\n",
    "        image_noisy = torch.from_numpy(image_batch.pic).squeeze().expand(1,1,-1,-1).float()\n",
    "        image_noisy = image_noisy.to(device)    \n",
    "        # Encode data\n",
    "        encoded_data = encoder(image_noisy)\n",
    "        # Decode data\n",
    "        decoded_data = decoder(encoded_data)\n",
    "        # Evaluate loss\n",
    "        loss = loss_fn(decoded_data, image_noisy)\n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # Print batch loss\n",
    "        train_loss.append(loss.detach().cpu().numpy())\n",
    "\n",
    "    return np.max(train_loss)\n",
    "\n",
    "def test_encode(encoder, decoder, device, dataloader, loss_fn):\n",
    "    # Set evaluation mode for encoder and decoder\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "    with torch.no_grad(): # No need to track the gradients\n",
    "        # Define the lists to store the outputs for each batch\n",
    "        losses = np.empty(len(dataloader))\n",
    "        for i, image_batch in enumerate(dataloader):\n",
    "            # Move tensor to the proper device\n",
    "            image_batch = torch.from_numpy(image_batch.pic).squeeze().expand(1,1,-1,-1).float()\n",
    "            # Encode data\n",
    "            encoded_data = encoder(image_batch)\n",
    "            # Decode data\n",
    "            decoded_data = decoder(encoded_data)\n",
    "            losses[i]=loss_fn(decoded_data.cpu(), image_batch.cpu())\n",
    "    return losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getMyFitnessBc(game, wVec, aVec, ind):\n",
    "    wVec[np.isnan(wVec)] = 0    \n",
    "    fitness, bc0 = myTestInd(game, wVec, aVec, ind)\n",
    "    ind.bc0 = bc0\n",
    "    return fitness, bc0\n",
    "\n",
    "def myTestInd(game, wVec, aVec, ind):\n",
    "    myPic = np.empty((32,32,1))      \n",
    "    for i in range(game.maxEpisodeLength): \n",
    "        myPic[i%32][int(i/32)] = pn.act(wVec, aVec, game.nInput, game.nOutput, x[i%32][int(i/32)])\n",
    "    ind.pic = myPic\n",
    "    return (np.square(goal-myPic).sum()-(32*32))/((-32*32))*100, aVec.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def redo_bc1(inds):\n",
    "    bc1s = test_encode(encoder, decoder, device, inds, loss_fn)\n",
    "    for i, ind in enumerate(inds):\n",
    "        ind.bc1=bc1s[i]\n",
    "        \n",
    "def save_elites(my_map, folder, size):\n",
    "    os.mkdir(folder)\n",
    "    elite_pic = np.empty(size)\n",
    "    pdic = {'bc0':[],'bc1':[],'fitness':[]}\n",
    "    for i in range(size[0]):\n",
    "        for j in range(size[1]):\n",
    "            key = i*size[1]+j\n",
    "            if key in my_map:\n",
    "                elite_pic[i][j]=my_map[key].fitness\n",
    "                pdic['bc0'].append(i)\n",
    "                pdic['bc1'].append(j)\n",
    "                pdic['fitness'].append(my_map[key].fitness)\n",
    "            else:\n",
    "                elite_pic[i][j]=np.nan\n",
    "    #grid_archive_heatmap(elite_pic,vmin=0, vmax=100)\n",
    "    #plt.tight_layout()\n",
    "    #plt.savefig(f'{my_map}/mapOelites.png')\n",
    "    ar = pd.DataFrame(data=pdic)\n",
    "    pics=[]\n",
    "    for i in range(6):\n",
    "        for j in range(6):\n",
    "            partion=ar.loc[\n",
    "                (ar[\"bc0\"]>(j*size[0]/6))&\n",
    "                (ar[\"bc0\"]<((j+1)*size[0]/6))&\n",
    "                (ar[\"bc1\"]>(size[1]-(i+1)*size[1]/6))&\n",
    "                (ar[\"bc1\"]<(size[1]-(i)*size[1]/6))]\n",
    "            if partion.empty:\n",
    "                pics.append(na)\n",
    "            else:\n",
    "                max_obj=partion[(partion[\"fitness\"].max()==partion[\"fitness\"])]\n",
    "                a = partion['bc0'].to_numpy()[0]\n",
    "                b = partion['bc1'].to_numpy()[0]\n",
    "                max_loc=a*size[1]+b\n",
    "                pics.append(my_map[max_loc].pic)\n",
    "\n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(\"Best Images\")\n",
    "    for i,p in enumerate(pics):\n",
    "        plt.subplot(6,6,i+1)\n",
    "        plt.imshow(p, cmap='Greys_r',vmin=0,vmax=1)\n",
    "    plt.savefig(str(f'{folder}/results.png'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training autoencoder\n",
      "finished autoencoder\n",
      "gen:0, max fitness:76.516, archive coverage:3.125%\n",
      "gen:25, max fitness:75.717, archive coverage:6.25%\n",
      "gen:50, max fitness:77.511, archive coverage:6.25%\n",
      "gen:75, max fitness:76.457, archive coverage:6.25%\n",
      "gen:100, max fitness:76.258, archive coverage:6.25%\n",
      "gen:125, max fitness:76.408, archive coverage:6.25%\n",
      "gen:150, max fitness:79.145, archive coverage:6.25%\n",
      "gen:175, max fitness:77.329, archive coverage:6.25%\n",
      "gen:200, max fitness:77.005, archive coverage:6.25%\n",
      "gen:225, max fitness:81.459, archive coverage:6.25%\n",
      "gen:250, max fitness:77.391, archive coverage:6.25%\n",
      "gen:275, max fitness:87.021, archive coverage:6.25%\n",
      "gen:300, max fitness:78.317, archive coverage:6.25%\n",
      "gen:325, max fitness:80.423, archive coverage:6.25%\n",
      "gen:350, max fitness:76.209, archive coverage:6.25%\n",
      "gen:375, max fitness:80.385, archive coverage:6.25%\n",
      "gen:400, max fitness:82.505, archive coverage:6.25%\n",
      "gen:425, max fitness:78.845, archive coverage:6.25%\n",
      "gen:450, max fitness:77.038, archive coverage:6.25%\n",
      "gen:475, max fitness:77.317, archive coverage:6.25%\n",
      "gen:500, max fitness:77.554, archive coverage:6.25%\n",
      "gen:525, max fitness:84.461, archive coverage:6.25%\n",
      "gen:550, max fitness:80.263, archive coverage:6.25%\n",
      "gen:575, max fitness:80.237, archive coverage:6.25%\n",
      "gen:600, max fitness:81.243, archive coverage:6.25%\n",
      "gen:625, max fitness:79.072, archive coverage:6.25%\n",
      "gen:650, max fitness:86.370, archive coverage:6.25%\n",
      "gen:675, max fitness:82.786, archive coverage:6.25%\n",
      "gen:700, max fitness:76.263, archive coverage:6.25%\n",
      "gen:725, max fitness:84.610, archive coverage:6.25%\n",
      "gen:750, max fitness:76.031, archive coverage:6.25%\n",
      "gen:775, max fitness:74.149, archive coverage:6.25%\n",
      "gen:800, max fitness:74.462, archive coverage:6.25%\n",
      "gen:825, max fitness:75.519, archive coverage:6.25%\n",
      "gen:850, max fitness:82.981, archive coverage:6.25%\n",
      "gen:875, max fitness:75.779, archive coverage:6.25%\n",
      "gen:900, max fitness:84.950, archive coverage:6.25%\n",
      "gen:925, max fitness:84.806, archive coverage:6.25%\n",
      "gen:950, max fitness:87.350, archive coverage:6.25%\n",
      "gen:975, max fitness:78.182, archive coverage:6.25%\n",
      "gen:1000, max fitness:77.453, archive coverage:6.25%\n",
      "gen:1025, max fitness:75.701, archive coverage:6.25%\n",
      "gen:1050, max fitness:74.277, archive coverage:6.25%\n",
      "gen:1075, max fitness:83.114, archive coverage:6.25%\n",
      "gen:1100, max fitness:79.664, archive coverage:6.25%\n",
      "gen:1125, max fitness:75.570, archive coverage:6.25%\n",
      "gen:1150, max fitness:74.383, archive coverage:6.25%\n",
      "gen:1175, max fitness:83.378, archive coverage:6.25%\n",
      "gen:1200, max fitness:79.249, archive coverage:6.25%\n",
      "gen:1225, max fitness:88.243, archive coverage:6.25%\n",
      "gen:1250, max fitness:77.414, archive coverage:6.25%\n",
      "gen:1275, max fitness:80.267, archive coverage:6.25%\n",
      "gen:1300, max fitness:87.244, archive coverage:6.25%\n",
      "gen:1325, max fitness:75.897, archive coverage:6.25%\n",
      "gen:1350, max fitness:87.328, archive coverage:6.25%\n",
      "gen:1375, max fitness:75.857, archive coverage:6.25%\n",
      "gen:1400, max fitness:80.402, archive coverage:6.25%\n",
      "gen:1425, max fitness:75.329, archive coverage:6.25%\n",
      "gen:1450, max fitness:77.186, archive coverage:6.25%\n",
      "gen:1475, max fitness:79.493, archive coverage:6.25%\n",
      "gen:1500, max fitness:83.998, archive coverage:6.25%\n",
      "gen:1525, max fitness:79.474, archive coverage:6.25%\n",
      "gen:1550, max fitness:80.929, archive coverage:6.25%\n",
      "gen:1575, max fitness:79.872, archive coverage:6.25%\n",
      "gen:1600, max fitness:81.892, archive coverage:6.25%\n",
      "gen:1625, max fitness:78.460, archive coverage:6.25%\n",
      "gen:1650, max fitness:77.852, archive coverage:6.25%\n",
      "gen:1675, max fitness:91.794, archive coverage:6.25%\n",
      "gen:1700, max fitness:75.991, archive coverage:6.25%\n",
      "gen:1725, max fitness:78.213, archive coverage:6.25%\n",
      "gen:1750, max fitness:79.211, archive coverage:6.25%\n",
      "gen:1775, max fitness:77.441, archive coverage:6.25%\n",
      "gen:1800, max fitness:82.859, archive coverage:6.25%\n",
      "gen:1825, max fitness:80.394, archive coverage:6.25%\n",
      "gen:1850, max fitness:76.297, archive coverage:6.25%\n",
      "gen:1875, max fitness:88.098, archive coverage:6.25%\n",
      "gen:1900, max fitness:76.472, archive coverage:6.25%\n",
      "gen:1925, max fitness:75.867, archive coverage:6.25%\n",
      "gen:1950, max fitness:77.347, archive coverage:6.25%\n",
      "gen:1975, max fitness:82.717, archive coverage:6.25%\n",
      "training autoencoder\n",
      "finished autoencoder\n",
      "gen:2000, max fitness:77.187, archive coverage:6.25%\n",
      "gen:2025, max fitness:73.360, archive coverage:6.25%\n",
      "gen:2050, max fitness:77.143, archive coverage:6.25%\n",
      "gen:2075, max fitness:78.396, archive coverage:6.25%\n",
      "gen:2100, max fitness:80.776, archive coverage:6.25%\n",
      "gen:2125, max fitness:79.442, archive coverage:6.25%\n",
      "gen:2150, max fitness:77.371, archive coverage:6.25%\n",
      "gen:2175, max fitness:76.507, archive coverage:6.25%\n",
      "gen:2200, max fitness:74.960, archive coverage:6.25%\n",
      "gen:2225, max fitness:85.733, archive coverage:12.109375%\n",
      "gen:2250, max fitness:78.406, archive coverage:12.890625%\n",
      "gen:2275, max fitness:80.369, archive coverage:17.96875%\n",
      "gen:2300, max fitness:80.796, archive coverage:18.359375%\n",
      "gen:2325, max fitness:82.195, archive coverage:18.359375%\n",
      "gen:2350, max fitness:91.910, archive coverage:18.359375%\n",
      "gen:2375, max fitness:81.318, archive coverage:18.359375%\n",
      "gen:2400, max fitness:81.701, archive coverage:18.359375%\n",
      "gen:2425, max fitness:84.224, archive coverage:18.75%\n",
      "gen:2450, max fitness:78.151, archive coverage:18.75%\n",
      "gen:2475, max fitness:82.393, archive coverage:18.75%\n",
      "gen:2500, max fitness:80.405, archive coverage:18.75%\n",
      "gen:2525, max fitness:81.295, archive coverage:18.75%\n",
      "gen:2550, max fitness:82.633, archive coverage:18.75%\n",
      "gen:2575, max fitness:82.526, archive coverage:18.75%\n",
      "gen:2600, max fitness:87.069, archive coverage:18.75%\n",
      "gen:2625, max fitness:82.390, archive coverage:18.75%\n",
      "gen:2650, max fitness:83.126, archive coverage:18.75%\n",
      "gen:2675, max fitness:92.612, archive coverage:18.75%\n",
      "gen:2700, max fitness:86.687, archive coverage:18.75%\n",
      "gen:2725, max fitness:80.821, archive coverage:18.75%\n",
      "gen:2750, max fitness:83.566, archive coverage:18.75%\n",
      "gen:2775, max fitness:80.607, archive coverage:18.75%\n",
      "gen:2800, max fitness:84.556, archive coverage:18.75%\n",
      "gen:2825, max fitness:87.804, archive coverage:18.75%\n",
      "gen:2850, max fitness:85.466, archive coverage:18.75%\n",
      "gen:2875, max fitness:84.535, archive coverage:18.75%\n",
      "gen:2900, max fitness:80.119, archive coverage:18.75%\n",
      "gen:2925, max fitness:92.976, archive coverage:18.75%\n",
      "gen:2950, max fitness:85.087, archive coverage:18.75%\n",
      "gen:2975, max fitness:81.691, archive coverage:18.75%\n",
      "gen:3000, max fitness:88.110, archive coverage:18.75%\n",
      "gen:3025, max fitness:83.597, archive coverage:18.75%\n",
      "gen:3050, max fitness:84.860, archive coverage:18.75%\n",
      "gen:3075, max fitness:80.265, archive coverage:18.75%\n",
      "gen:3100, max fitness:83.819, archive coverage:18.75%\n",
      "gen:3125, max fitness:86.169, archive coverage:18.75%\n",
      "gen:3150, max fitness:87.057, archive coverage:18.75%\n",
      "gen:3175, max fitness:84.553, archive coverage:18.75%\n",
      "gen:3200, max fitness:81.744, archive coverage:18.75%\n",
      "gen:3225, max fitness:86.076, archive coverage:18.75%\n",
      "gen:3250, max fitness:79.342, archive coverage:18.75%\n",
      "gen:3275, max fitness:83.240, archive coverage:18.75%\n",
      "gen:3300, max fitness:83.898, archive coverage:18.75%\n",
      "gen:3325, max fitness:86.964, archive coverage:18.75%\n",
      "gen:3350, max fitness:83.563, archive coverage:18.75%\n",
      "gen:3375, max fitness:80.333, archive coverage:18.75%\n",
      "gen:3400, max fitness:84.742, archive coverage:18.75%\n",
      "gen:3425, max fitness:84.630, archive coverage:18.75%\n",
      "gen:3450, max fitness:86.644, archive coverage:18.75%\n",
      "gen:3475, max fitness:84.624, archive coverage:18.75%\n",
      "gen:3500, max fitness:84.478, archive coverage:18.75%\n",
      "gen:3525, max fitness:75.728, archive coverage:18.75%\n",
      "gen:3550, max fitness:82.698, archive coverage:18.75%\n",
      "gen:3575, max fitness:81.313, archive coverage:18.75%\n",
      "gen:3600, max fitness:78.761, archive coverage:18.75%\n",
      "gen:3625, max fitness:86.440, archive coverage:18.75%\n",
      "gen:3650, max fitness:84.467, archive coverage:18.75%\n",
      "gen:3675, max fitness:82.927, archive coverage:18.75%\n",
      "gen:3700, max fitness:79.648, archive coverage:18.75%\n",
      "gen:3725, max fitness:80.400, archive coverage:18.75%\n",
      "gen:3750, max fitness:78.928, archive coverage:18.75%\n",
      "gen:3775, max fitness:87.947, archive coverage:18.75%\n",
      "gen:3800, max fitness:89.401, archive coverage:18.75%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gen:3825, max fitness:78.186, archive coverage:18.75%\n",
      "gen:3850, max fitness:85.418, archive coverage:18.75%\n",
      "gen:3875, max fitness:82.413, archive coverage:18.75%\n",
      "gen:3900, max fitness:86.624, archive coverage:18.75%\n",
      "gen:3925, max fitness:88.301, archive coverage:18.75%\n",
      "gen:3950, max fitness:79.577, archive coverage:18.75%\n",
      "gen:3975, max fitness:88.187, archive coverage:18.75%\n",
      "training autoencoder\n",
      "finished autoencoder\n",
      "gen:4000, max fitness:92.539, archive coverage:10.9375%\n",
      "gen:4025, max fitness:86.306, archive coverage:18.359375%\n",
      "gen:4050, max fitness:76.769, archive coverage:18.75%\n",
      "gen:4075, max fitness:89.553, archive coverage:18.75%\n",
      "gen:4100, max fitness:79.362, archive coverage:18.75%\n",
      "gen:4125, max fitness:86.333, archive coverage:18.75%\n",
      "gen:4150, max fitness:88.990, archive coverage:18.75%\n",
      "gen:4175, max fitness:83.054, archive coverage:18.75%\n",
      "gen:4200, max fitness:83.381, archive coverage:18.75%\n",
      "gen:4225, max fitness:83.021, archive coverage:18.75%\n",
      "gen:4250, max fitness:91.445, archive coverage:18.75%\n",
      "gen:4275, max fitness:89.471, archive coverage:18.75%\n",
      "gen:4300, max fitness:92.823, archive coverage:18.75%\n",
      "gen:4325, max fitness:87.104, archive coverage:18.75%\n",
      "gen:4350, max fitness:88.730, archive coverage:18.75%\n",
      "gen:4375, max fitness:86.566, archive coverage:18.75%\n",
      "gen:4400, max fitness:77.064, archive coverage:18.75%\n",
      "gen:4425, max fitness:80.341, archive coverage:18.75%\n",
      "gen:4450, max fitness:84.655, archive coverage:18.75%\n",
      "gen:4475, max fitness:92.341, archive coverage:18.75%\n",
      "gen:4500, max fitness:82.435, archive coverage:18.75%\n",
      "gen:4525, max fitness:79.209, archive coverage:18.75%\n",
      "gen:4550, max fitness:73.456, archive coverage:18.75%\n",
      "gen:4575, max fitness:80.862, archive coverage:18.75%\n",
      "gen:4600, max fitness:85.682, archive coverage:18.75%\n",
      "gen:4625, max fitness:85.601, archive coverage:18.75%\n",
      "gen:4650, max fitness:84.120, archive coverage:18.75%\n",
      "gen:4675, max fitness:91.501, archive coverage:18.75%\n",
      "gen:4700, max fitness:79.648, archive coverage:18.75%\n",
      "gen:4725, max fitness:82.668, archive coverage:18.75%\n",
      "gen:4750, max fitness:93.205, archive coverage:18.75%\n",
      "gen:4775, max fitness:84.614, archive coverage:18.75%\n",
      "gen:4800, max fitness:87.369, archive coverage:18.75%\n",
      "gen:4825, max fitness:84.181, archive coverage:18.75%\n",
      "gen:4850, max fitness:86.986, archive coverage:18.75%\n",
      "gen:4875, max fitness:90.078, archive coverage:18.75%\n",
      "gen:4900, max fitness:81.049, archive coverage:18.75%\n",
      "gen:4925, max fitness:91.593, archive coverage:18.75%\n",
      "gen:4950, max fitness:85.725, archive coverage:18.75%\n",
      "gen:4975, max fitness:79.678, archive coverage:18.75%\n",
      "gen:5000, max fitness:81.670, archive coverage:18.75%\n",
      "gen:5025, max fitness:91.030, archive coverage:18.75%\n",
      "gen:5050, max fitness:87.179, archive coverage:18.75%\n",
      "gen:5075, max fitness:85.183, archive coverage:18.75%\n",
      "gen:5100, max fitness:84.467, archive coverage:18.75%\n",
      "gen:5125, max fitness:87.227, archive coverage:18.75%\n",
      "gen:5150, max fitness:89.151, archive coverage:18.75%\n",
      "gen:5175, max fitness:86.124, archive coverage:18.75%\n",
      "gen:5200, max fitness:87.478, archive coverage:18.75%\n",
      "gen:5225, max fitness:85.700, archive coverage:18.75%\n",
      "gen:5250, max fitness:87.474, archive coverage:18.75%\n",
      "gen:5275, max fitness:80.405, archive coverage:18.75%\n",
      "gen:5300, max fitness:90.778, archive coverage:18.75%\n",
      "gen:5325, max fitness:82.427, archive coverage:18.75%\n",
      "gen:5350, max fitness:80.115, archive coverage:18.75%\n",
      "gen:5375, max fitness:79.891, archive coverage:18.75%\n",
      "gen:5400, max fitness:84.847, archive coverage:18.75%\n",
      "gen:5425, max fitness:87.929, archive coverage:18.75%\n",
      "gen:5450, max fitness:89.849, archive coverage:18.75%\n",
      "gen:5475, max fitness:83.191, archive coverage:18.75%\n",
      "gen:5500, max fitness:76.717, archive coverage:18.75%\n",
      "gen:5525, max fitness:81.870, archive coverage:18.75%\n",
      "gen:5550, max fitness:80.226, archive coverage:18.75%\n",
      "gen:5575, max fitness:78.387, archive coverage:18.75%\n",
      "gen:5600, max fitness:81.251, archive coverage:18.75%\n",
      "gen:5625, max fitness:81.656, archive coverage:18.75%\n",
      "gen:5650, max fitness:85.963, archive coverage:18.75%\n",
      "gen:5675, max fitness:86.685, archive coverage:18.75%\n",
      "gen:5700, max fitness:78.212, archive coverage:18.75%\n",
      "gen:5725, max fitness:84.260, archive coverage:18.75%\n",
      "gen:5750, max fitness:84.293, archive coverage:18.75%\n",
      "gen:5775, max fitness:91.942, archive coverage:18.75%\n",
      "gen:5800, max fitness:84.473, archive coverage:18.75%\n",
      "gen:5825, max fitness:78.661, archive coverage:18.75%\n",
      "gen:5850, max fitness:79.158, archive coverage:18.75%\n",
      "gen:5875, max fitness:81.775, archive coverage:18.75%\n",
      "gen:5900, max fitness:79.477, archive coverage:18.75%\n",
      "gen:5925, max fitness:81.641, archive coverage:18.75%\n",
      "gen:5950, max fitness:88.342, archive coverage:18.75%\n",
      "gen:5975, max fitness:86.847, archive coverage:18.75%\n",
      "training autoencoder\n",
      "finished autoencoder\n",
      "gen:6000, max fitness:88.464, archive coverage:12.5%\n",
      "gen:6025, max fitness:89.611, archive coverage:17.96875%\n",
      "gen:6050, max fitness:76.893, archive coverage:17.96875%\n",
      "gen:6075, max fitness:80.212, archive coverage:17.96875%\n",
      "gen:6100, max fitness:79.287, archive coverage:17.96875%\n",
      "gen:6125, max fitness:76.980, archive coverage:17.96875%\n",
      "gen:6150, max fitness:81.112, archive coverage:17.96875%\n",
      "gen:6175, max fitness:87.071, archive coverage:17.96875%\n",
      "gen:6200, max fitness:89.007, archive coverage:17.96875%\n",
      "gen:6225, max fitness:85.695, archive coverage:17.96875%\n",
      "gen:6250, max fitness:80.255, archive coverage:17.96875%\n",
      "gen:6275, max fitness:89.673, archive coverage:17.96875%\n",
      "gen:6300, max fitness:83.642, archive coverage:17.96875%\n",
      "gen:6325, max fitness:89.893, archive coverage:17.96875%\n",
      "gen:6350, max fitness:89.936, archive coverage:17.96875%\n",
      "gen:6375, max fitness:86.070, archive coverage:17.96875%\n",
      "gen:6400, max fitness:89.081, archive coverage:17.96875%\n",
      "gen:6425, max fitness:89.397, archive coverage:17.96875%\n",
      "gen:6450, max fitness:79.615, archive coverage:17.96875%\n",
      "gen:6475, max fitness:81.552, archive coverage:17.96875%\n",
      "gen:6500, max fitness:82.724, archive coverage:17.96875%\n",
      "gen:6525, max fitness:86.172, archive coverage:17.96875%\n",
      "gen:6550, max fitness:81.266, archive coverage:17.96875%\n",
      "gen:6575, max fitness:80.520, archive coverage:17.96875%\n",
      "gen:6600, max fitness:82.905, archive coverage:17.96875%\n",
      "gen:6625, max fitness:79.988, archive coverage:17.96875%\n",
      "gen:6650, max fitness:78.303, archive coverage:17.96875%\n",
      "gen:6675, max fitness:82.226, archive coverage:17.96875%\n",
      "gen:6700, max fitness:79.814, archive coverage:17.96875%\n",
      "gen:6725, max fitness:91.720, archive coverage:17.96875%\n",
      "gen:6750, max fitness:87.064, archive coverage:17.96875%\n",
      "gen:6775, max fitness:87.826, archive coverage:17.96875%\n",
      "gen:6800, max fitness:77.095, archive coverage:17.96875%\n",
      "gen:6825, max fitness:82.431, archive coverage:17.96875%\n",
      "gen:6850, max fitness:87.580, archive coverage:17.96875%\n",
      "gen:6875, max fitness:90.248, archive coverage:17.96875%\n",
      "gen:6900, max fitness:82.828, archive coverage:17.96875%\n",
      "gen:6925, max fitness:79.118, archive coverage:17.96875%\n",
      "gen:6950, max fitness:90.381, archive coverage:17.96875%\n",
      "gen:6975, max fitness:84.598, archive coverage:17.96875%\n",
      "gen:7000, max fitness:88.660, archive coverage:17.96875%\n",
      "gen:7025, max fitness:81.989, archive coverage:17.96875%\n",
      "gen:7050, max fitness:83.649, archive coverage:17.96875%\n",
      "gen:7075, max fitness:89.450, archive coverage:17.96875%\n",
      "gen:7100, max fitness:83.362, archive coverage:17.96875%\n",
      "gen:7125, max fitness:85.209, archive coverage:17.96875%\n",
      "gen:7150, max fitness:90.237, archive coverage:17.96875%\n",
      "gen:7175, max fitness:90.259, archive coverage:17.96875%\n",
      "gen:7200, max fitness:84.487, archive coverage:17.96875%\n",
      "gen:7225, max fitness:92.685, archive coverage:17.96875%\n",
      "gen:7250, max fitness:84.631, archive coverage:17.96875%\n",
      "gen:7275, max fitness:84.397, archive coverage:17.96875%\n",
      "gen:7300, max fitness:84.675, archive coverage:17.96875%\n",
      "gen:7325, max fitness:90.414, archive coverage:17.96875%\n",
      "gen:7350, max fitness:87.217, archive coverage:17.96875%\n",
      "gen:7375, max fitness:93.068, archive coverage:17.96875%\n",
      "gen:7400, max fitness:84.780, archive coverage:17.96875%\n",
      "gen:7425, max fitness:88.911, archive coverage:17.96875%\n",
      "gen:7450, max fitness:92.689, archive coverage:18.359375%\n",
      "gen:7475, max fitness:84.685, archive coverage:18.359375%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gen:7500, max fitness:82.153, archive coverage:18.359375%\n",
      "gen:7525, max fitness:79.372, archive coverage:18.359375%\n",
      "gen:7550, max fitness:91.029, archive coverage:18.359375%\n",
      "gen:7575, max fitness:89.842, archive coverage:18.359375%\n",
      "gen:7600, max fitness:82.103, archive coverage:18.359375%\n",
      "gen:7625, max fitness:89.851, archive coverage:18.359375%\n",
      "gen:7650, max fitness:82.746, archive coverage:18.359375%\n",
      "gen:7675, max fitness:86.451, archive coverage:18.359375%\n",
      "gen:7700, max fitness:83.396, archive coverage:18.359375%\n",
      "gen:7725, max fitness:87.636, archive coverage:18.359375%\n",
      "gen:7750, max fitness:82.069, archive coverage:18.359375%\n",
      "gen:7775, max fitness:81.078, archive coverage:18.359375%\n",
      "gen:7800, max fitness:84.118, archive coverage:18.359375%\n"
     ]
    }
   ],
   "source": [
    "folder= \"simple2\"\n",
    "neat = NeatME(hyp)\n",
    "mymapsize=neat.p[\"map_size\"][0]*neat.p[\"map_size\"][1]\n",
    "for gen in range(hyp['maxGen']):\n",
    "    pop = neat.ask()\n",
    "# Get newly evolved individuals from NEAT\n",
    "    reward = np.empty(len(pop), dtype=np.float64)\n",
    "    bc0 = np.empty(len(pop), dtype=np.float64)\n",
    "    archive = []\n",
    "    q = 0\n",
    "    for i in range(len(pop)):\n",
    "        task = domain.GymTask(game, paramOnly=True, nReps=hyp['alg_nReps'])\n",
    "        wVec   = pop[i].wMat.flatten()\n",
    "        aVec   = pop[i].aVec.flatten()\n",
    "        reward[i], bc0[i] = getMyFitnessBc(task, wVec, aVec, pop[i]) # process it\n",
    "    a_len = len(archive)\n",
    "    #make full archive\n",
    "    if len(archive)>5000:\n",
    "        for po in pop:\n",
    "            archive[q]=po\n",
    "            q = (q+1)%a_len\n",
    "    else:\n",
    "        for po in pop:\n",
    "            archive.append(po)\n",
    "    if gen % 2000 == 0:\n",
    "        max_loss = 0\n",
    "        print('training autoencoder')\n",
    "        for i in range(250):\n",
    "            max_loss = train_epoch_den(encoder, decoder, device, archive, loss_fn, optim)\n",
    "        print('finished autoencoder')\n",
    "        neat.p['bc_range'][1][1]=max_loss\n",
    "        elites=[*neat.map_elites.values()]\n",
    "        neat.map_elites={}\n",
    "        redo_bc1(archive)\n",
    "        redo_bc1(elites)\n",
    "        neat.tell(reward, bc0, test_encode(encoder, decoder, device, neat.pop, loss_fn))\n",
    "        neat.fill_elites(elites)\n",
    "        neat.fill_elites(archive)\n",
    "    else:\n",
    "        neat.tell(reward, bc0, test_encode(encoder, decoder, device, neat.pop, loss_fn))\n",
    "    neat.gen += 1\n",
    "    if gen % 25 == 0:\n",
    "        my_max=(np.max(reward))\n",
    "        cov = len(neat.map_elites)/mymapsize*100\n",
    "        print(f'gen:{gen}, max fitness:{my_max:.3f}, archive coverage:{cov}%')\n",
    "        if my_max > 99.5:\n",
    "            print(my_max)\n",
    "            break\n",
    "save_elites(neat.map_elites, folder, neat.p['map_size'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
